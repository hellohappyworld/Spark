```scala

**************************************************(算子进阶)**************************************************
-----------------------------------------------------------------------------------------------------------
---练习1  手动指定分区数
-----------------------------------------------------------------------------------------------------------
生成RDD方法
scala> val rdd1=sc.parallelize(List(1,2,3,4,5,6))
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27


val rdd2=sc.textFile("hdfs://min1:9000/wc")


rdd分区数
scala> rdd1.partitions.length
res0: Int = 2

手动指定分区数
scala> val rdd1=sc.parallelize(List(1,2,3,4,5,6),10)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[48] at parallelize at <console>:27

scala> rdd1.partitions.length
res41: Int = 10


scala> val rdd2=sc.textFile("hdfs://min1:9000/wc",4)
rdd2: org.apache.spark.rdd.RDD[String] = hdfs://min1:9000/wc MapPartitionsRDD[52] at textFile at <console>:27

scala> rdd2.partitions.length
res42: Int = 4



-----------------------------------------------------------------------------------------------------------
---练习2（mapPartitions）
-----------------------------------------------------------------------------------------------------------

scala> val rdd1=sc.parallelize(Array(1,2,3,4,5,6))
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:21

scala> rdd1.partitions.length
res43: Int = 2

mapPartitions得到的是一个分区，map是得到一个元素
scala> val rdd2=rdd1.mapPartitions(_.map(_*10))
rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at mapPartitions at <console>:23

scala> rdd2.collect
res44: Array[Int] = Array(10, 20, 30, 40, 50, 60) 


-----------------------------------------------------------------------------------------------------------
---练习3（mapWith）
-----------------------------------------------------------------------------------------------------------
参数列表：(constructA: Int => A, preservesPartitioning: Boolean = false)(f: (T, A) => U)
mapWith(分区索引对应函数，是否保存分区信息)（f:(T,A)=>U）       T:每次拿到RDD中一个元素  A:分区索引对应函数结果(其中分区索引为0,1,2,3.....)

scala> val rdd1=sc.parallelize(Array(1,2,3,4,5,6))
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27

scala> rdd1.mapWith(i => i*10,true)((a,b) => b+2).collect
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
res0: Array[Int] = Array(2, 2, 2, 12, 12, 12)   

-----------------------------------------------------------------------------------------------------------
---练习4（flatMapWith）
-----------------------------------------------------------------------------------------------------------
参数列表：(constructA: Int => A, preservesPartitioning: Boolean = false)(f: (T, A) => Seq[U])
flatMapWith(分区索引对应函数，是否保存分区信息)((f: (T, A) => Seq[U]))   T:每次拿到RDD中的一个元素  A:分区索引对应函数计算结果

将每个元素与其对应的分区拼接成一个元组
scala> rdd1.flatMapWith(i => i,true)((x,y)=>List((y,x))).collect
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
res3: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (1,4), (1,5), (1,6))


-----------------------------------------------------------------------------------------------------------
---练习5（mapPartitionsWithIndex）
-----------------------------------------------------------------------------------------------------------
参数列表：(f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false)
mapPartitionsWithIndex(f:(Int,Iterator[T]))   第一个参数为分区索引，T为分区中相应元素


scala> val rdd1=sc.parallelize(List(1,2,3,4,5,6,7,8,9),2)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[58] at parallelize at <console>:27

创建函数func，将id号和每个元素打印出来（也就是能查看出RDD中的每个元素分别在哪个分区）
scala> val func = (index: Int, iter: Iterator[(Int)]) => {
     |   iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
     | }
func: (Int, Iterator[Int]) => Iterator[String] = <function2>

scala> rdd1.mapPartitionsWithIndex(func).collect
res47: Array[String] = Array([partID:0, val: 1], [partID:0, val: 2], [partID:0, val: 3], [partID:0, val: 4], [partID:1, val: 5], [partID:1, val: 6], [partID:1, val: 7], [partID:1, val: 8], [partID:1, val: 9])

创建函数func1，将id号和每个元素打印出来（也就是能查看出RDD中的每个元素分别在哪个分区）
scala> val func1=(index:Int,iter:Iterator[(Int)]) => {iter.toList.map(x => "{Id:"+index+",yuansu:"+x+"}").iterator}
func1: (Int, Iterator[Int]) => Iterator[String] = <function2>

scala> rdd1.mapPartitionsWithIndex(func1).collect
res50: Array[String] = Array({Id:0,yuansu:1}, {Id:0,yuansu:2}, {Id:0,yuansu:3}, {Id:0,yuansu:4}, {Id:1,yuansu:5}, {Id:1,yuansu:6}, {Id:1,yuansu:7}, {Id:1,yuansu:8}, {Id:1,yuansu:9})


-----------------------------------------------------------------------------------------------------------
---练习6(aggregate)
-----------------------------------------------------------------------------------------------------------
参数列表：(zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U     其中T为RDD中元素
aggregate    (初始值)（局部聚合，全局聚合）      

scala> def func1(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
     |   iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
     | }
func1: (index: Int, iter: Iterator[Int])Iterator[String]

scala> val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at <console>:27

查看每个元素所在分区（元素1,2,3,4在0分区，，元素5,6,7,8,9在1分区）
scala> rdd1.mapPartitionsWithIndex(func1).collect
res53: Array[String] = Array([partId:0,val:1], [partId:0,val:2], [partId:0,val:3], [partId:0,val:4], [partId:1,val:5], [partId:1,val:6], [partId:1,val:7], [partId:1,val:8], [partId:1,val:9])

scala> rdd1.aggregate(0)(math.max(_,_),_+_)
res5: Int = 13
分析：
分区0：
初始值：0
局部聚合：0先跟1比较大小得1，1再跟2比较得2,2再跟3比较得3,3跟4比较得4，此时0分区中元素比较完毕，最终结果为4

分区1：
初始值：1
局部聚合：0先跟5比较大小得5,5跟6比较得6,6跟7比较得7,7跟8比较得8,8跟9比较得9，此时1分区中元素比较完毕，最终结果为9

全局聚合：_+_   第一个下划线代表上一个计算结果，第二个下划线代表当前元素
也就是：0先跟0分区结果4相加得到4，然后4再和1分区结果9相加得到13，，因此最终结果为13



同理分析下述代码：
scala> rdd1.aggregate(5)(math.max(_,_),_+_)
res6: Int = 19           


-----------------------------------------------------------------------------------------------------------
---练习7(aggregate)
-----------------------------------------------------------------------------------------------------------

scala> val rdd2 = sc.parallelize(List("a","b","c","d","e","f"),2)
rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[6] at parallelize at <console>:27

scala> def func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = {
     |   iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
     | }
func2: (index: Int, iter: Iterator[String])Iterator[String]

查看RDD中元素具体分区
scala> rdd2.mapPartitionsWithIndex(func2).collect
res59: Array[String] = Array([partID:0,val:a], [partID:0,val:b], [partID:0,val:c], [partID:1,val:d], [partID:1,val:e], [partID:1,val:f])

scala> rdd2.aggregate("")(_ + _, _ + _)
res7: String = abcdef   

scala> rdd2.aggregate("")(_ + _, _ + _)
res10: String = defabc      因为两个分区结果是abc def  而拼接顺序可能不同，，分区0的局部拼接为abc,分区1的局部拼接为def，但当全局拼接时拼接顺序不同时便会发生这两种不同的情况

scala> rdd2.aggregate("=")(_ + _, _ + _)
res11: String = ==abc=def
分析：
= 初始值
0:a,b,c
局部拼接：=abc

1:d,e,f
局部拼接:=def

全局拼接：(第一个等于号代表的是初始值)
==abc=def

-----------------------------------------------------------------------------------------------------------
---练习8(aggregate)
-----------------------------------------------------------------------------------------------------------

scala> val rdd3 = sc.parallelize(List("12","23","345","4567"),2)
rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:27

创建函数用于查看RDD中元素属于哪个分区
scala> def func(index:Int,iter:Iterator[(String)]):Iterator[String]={
     | iter.toList.map(x=>"[id:"+index+",val:"+x+"]").iterator
     | }
func: (index: Int, iter: Iterator[String])Iterator[String]

scala> rdd3.mapPartitionsWithIndex(func).collect
res0: Array[String] = Array([id:0,val:12], [id:0,val:23], [id:1,val:345], [id:1,val:4567])

该aggregate的功能是求出每个分区中元素长度最长的那个长度值，然后将不同分区得到的结果进行拼接形成新的字符串
scala> rdd3.aggregate("")((x,y) => math.max(x.length, y.length).toString, (x,y) => x + y)
res0: String = 42         
scala> rdd3.aggregate("")((x,y) => math.max(x.length, y.length).toString, (x,y) => x + y)
res1: String = 24
分析：
0：“12”，“23”
局部：“2”和“2”比较
“2”

1：“345”，“4567”
局部：“3”和“4”比较
“4”

全局：“24”或“42”


-----------------------------------------------------------------------------------------------------------
---练习9(aggregate)
-----------------------------------------------------------------------------------------------------------
scala> val rdd4 = sc.parallelize(List("12","23","345",""),2)
rdd4: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at <console>:27

scala> rdd4.aggregate("")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y)
res3: String = 01

scala> rdd4.aggregate("")((x,y)=>math.min(x.length,y.length).toString,(x,y)=>x+y)
res6: String = 10

初始值：“”
0：“12”，“23”
局部：一开始“”的长度与“12”比较，得最小结果为“0”    然后“0”的长度与“23”长度比较得“1”

1：“345”，“”
同上：最后结果为“0”


-----------------------------------------------------------------------------------------------------------
---练习10(aggregate)
-----------------------------------------------------------------------------------------------------------
scala> val rdd5 = sc.parallelize(List("12","23","","345"),2)
rdd5: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at <console>:27

scala> rdd5.aggregate("")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y)
res4: String = 11


-----------------------------------------------------------------------------------------------------------
---练习11(aggregateByKey)
-----------------------------------------------------------------------------------------------------------
参数列表：(zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)
---aggregateByKey(初始值，分区器)(（初始值，拿到每个元素），（全局逻辑）)


scala> val pairRDD = sc.parallelize(List( ("cat",2), ("cat", 5), ("mouse", 4),("cat", 12), ("dog", 12), ("mouse", 2)), 2)
pairRDD: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[3] at parallelize at <console>:27

scala> def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {
     |   iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
     | }
func2: (index: Int, iter: Iterator[(String, Int)])Iterator[String]

scala> pairRDD.mapPartitionsWithIndex(func2).collect
res5: Array[String] = Array([partID:0, val: (cat,2)], [partID:0, val: (cat,5)], [partID:0, val: (mouse,4)], [partID:1, val: (cat,12)], [partID:1, val: (dog,12)], [partID:1, val: (mouse,2)])

scala> pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect
res6: Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6))  
对于每个分区中的元组先按照key进行分组，然后先和初始值比较，然后把值再与下个
初始值：0
0：("cat",2) ("cat", 5)                 ("mouse", 4)     
局部：0先和("cat",2)得2     2和("cat", 5）中的5比较得5
      0和("mouse", 4)比较得4
      
      最后结果：("cat", 5），("mouse", 4)

1：("cat", 12)                ("dog", 12)                ("mouse", 2)
同上：("cat", 12) ， ("dog", 12)  ，("mouse", 2)

全局聚合：(dog,12), (cat,17), (mouse,6)


scala> pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).collect
res7: Array[(String, Int)] = Array((dog,100), (cat,200), (mouse,200))

局部：("cat",100)                 ("mouse", 100) 
局部：("cat", 100) ， ("dog", 100)  ，("mouse", 100）
全局：(dog,100), (cat,200), (mouse,200)

-----------------------------------------------------------------------------------------------------------
---练习12(combineByKey)
-----------------------------------------------------------------------------------------------------------
参数列表：(createCombiner: V => C, mergeValue: (C, V) => C, mergeCombiners: (C, C) => C)
首先先将每个分区中每个元组按照key分组，     createCombiner: V => C代表将分组后的元组中的第一个value取出，然后进行函数运算（结果也就是初始值）    mergeValue: (C, V) => C代表首先将初始值与对应元组中下一个value运算，结果作为初始值      mergeCombiners: (C, C) => C代表将每一个分区中的元组按照key值将value运算

scala> val rdd1=sc.textFile("hdfs://min1:9000/wc").flatMap(_.split(" ")).map((_,1))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[11] at map at <console>:27

scala> rdd1.collect
res12: Array[(String, Int)] = Array((hello,1), (tom,1), (hello,1), (jerry,1), (hello,1), (lilei,1), (hello,1), (hanmeimei,1), (hello,1), (tom,1), (hello,1), (tom,1), (hello,1), (jerry,1), (hello,1), (tom,1))

用于查看每个元组所处分区
scala> def func(index:Int,iter:Iterator[(String,Int)]):Iterator[String]={
     | iter.toList.map(x=>"[id:"+index+",val:"+x+"]").iterator
     | }
func: (index: Int, iter: Iterator[(String, Int)])Iterator[String]

scala> rdd1.mapPartitionsWithIndex(func).collect
res3: Array[String] = Array([id:0,val:(hello,1)], [id:0,val:(tom,1)], [id:0,val:(hello,1)], [id:0,val:(jerry,1)], [id:0,val:(hello,1)], [id:0,val:(lilei,1)], [id:0,val:(hello,1)], [id:0,val:(hanmeimei,1)], 
                            [id:1,val:(hello,1)], [id:1,val:(tom,1)], [id:1,val:(hello,1)], [id:1,val:(tom,1)], [id:1,val:(hello,1)], [id:1,val:(jerry,1)], [id:1,val:(hello,1)], [id:1,val:(tom,1)])

scala> val rdd2=rdd1.combineByKey(x=>x,(a:Int,b:Int)=>a+b,(m:Int,n:Int)=>m+n)
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[12] at combineByKey at <console>:29

scala> rdd2.collect
res13: Array[(String, Int)] = Array((tom,4), (hello,8), (jerry,2), (hanmeimei,1), (lilei,1))

分析：
0分区原始数据：
hello,1
tom,1
hello,1
jerry,1
hello,1
lilei,1
hello,1
hanmeimei,1

按key分组后：
hello:{1,1,1,1}
tom:{1}
jerry:{1}
lilei:{1}
hanmeimei:{1}

首先执行x=>x，也就是将hello组中第一个value作为初始值；x=1 ，，其它分组类似
然后执行(a:Int,b:Int)=>a+b  此时a的值也就是初始值x=1，然后a与hello元组中第二个1相加后赋给a，然后a与hello元组中第三个1相加后赋给a,然后与第四个1相加后得到最终结果：4
其它分组类似，当其它分组按照(a:Int,b:Int)=>a+b分别都执行完后，局部运算结束，此时0分区的局部结果为：
hello,4
tom,1
jerry,1
lilei,1
hanmeimei,1

下面是1分区：
原始数据：
hello,1
tom,1
hello,1
tom,1
hello,1
jerry,1
hello,1
tom,1

按key分组后：
tom:{1,1,1}
hello:{1,1,1,1}
jerry:{1}

首先执行x=>x,也就是将tom组中第一个value作为初始值：x=1，，其它两个分组一样
然后执行(a:Int,b:Int)=>a+b，一开始是将初始值x的值赋给a,然后a与tom分组中的第二个value(b=1)相加，然后结果赋给a,然后将a与第三个value(b=1)相加，，得到最终结果：3
其它分组一样运算过程，当每个分组都运算结束后，此时1分区的局部结果为：
tom,3
hello,4
jerry,1

最最最后，也就是开始执行(m:Int,n:Int)=>m+n，这是将每个分区中的key相同的元组的值相加，得到最终结果：
(tom,4), (hello,8), (jerry,2), (hanmeimei,1), (lilei,1)






scala> val rdd3=rdd1.combineByKey(x => x*10,(a:Int,b:Int)=>a+b,(m:Int,n:Int)=>m+n)
rdd3: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[9] at combineByKey at <console>:29

scala> rdd3.collect
res4: Array[(String, Int)] = Array((tom,22), (hello,26), (jerry,20), (hanmeimei,10), (lilei,10))
分析：
0分区原始数据：
hello,1
tom,1
hello,1
jerry,1
hello,1
lilei,1
hello,1
hanmeimei,1

按key分组后：
hello:{1,1,1,1}
tom:{1}
jerry:{1}
lilei:{1}
hanmeimei:{1}

首先执行的是x => x*10，也就是将每一个分组的第一个value与10相乘作为初始值，此时hello组的初始值为10，其它分组初始值同样也是10
然后执行(a:Int,b:Int)=>a+b，，一开始会将初始值x赋给a,然后将a与每个分组的第二个value(也就b)相加，然后结果重新赋给a，然后与第三个值相加，一直这样直到分组中数据都被处理完，
此时局部逻辑运算结束，各元组结果：{hello,13}   {tom,10}  {jerry,10}  {lilei,10}  {hanmeimei,10}

下面是1分区：
原始数据：
hello,1
tom,1
hello,1
tom,1
hello,1
jerry,1
hello,1
tom,1

按key分组后：
tom:{1,1,1}
hello:{1,1,1,1}
jerry:{1}

首先执行的是x => x*10，也就是将每一个分组的第一个value与10相乘作为初始值，此时tom组的初始值为10，其它分组初始值同样也是10
然后执行(a:Int,b:Int)=>a+b，，一开始会将初始值x赋给a,然后将a与每个分组的第二个value(也就b)相加，然后结果重新赋给a，然后与第三个值相加，一直这样直到分组中数据都被处理完，
此时局部逻辑运算结束，各元组结果：{hello,13}  {tom,12}  {jerry,10}

最最最后，也就是开始执行(m:Int,n:Int)=>m+n，这是将每个分区中的key相同的元组的值相加，得到最终结果：
(tom,22), (hello,26), (jerry,20), (hanmeimei,10), (lilei,10)


-----------------------------------------------------------------------------------------------------------
---练习13(combineByKey)
-----------------------------------------------------------------------------------------------------------
scala> val rdd4=sc.parallelize(List("dog","cat","salmon","rabbit","turkey","wolf","bear","bee"),3)
rdd4: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at <console>:27

scala> val rdd5=sc.parallelize(List(1,1,2,2,2,1,2,2,2),3)
rdd5: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at parallelize at <console>:27

拉链操作
scala> val rdd6 = rdd5.zip(rdd4)
rdd6: org.apache.spark.rdd.RDD[(Int, String)] = ZippedPartitionsRDD2[19] at zip at <console>:31

scala> rdd6.collect
res14: Array[(Int, String)] = Array((1,dog), (1,cat), (2,gnu), (2,salmon), (2,rabbit), (1,turkey), (2,wolf), (2,bear), (2,bee))

注意这次的初始值为：List(_)      局部运算： x :+ y  也就是每一个元组y添加到链表x中    全局运算：m ++ n 就是按照key值将相同key值下的value链表拼接到一起
val rdd7 = rdd6.combineByKey(List(_), (x: List[String], y: String) => x :+ y, (m: List[String], n: List[String]) => m ++ n)

scala> rdd7.collect
res13: Array[(Int, List[String])] = Array((1,List(turkey, dog, cat)), (2,List(gnu, wolf, bear, bee, salmon, rabbit)))


-----------------------------------------------------------------------------------------------------------
---练习14(countByKey,countByValue)
-----------------------------------------------------------------------------------------------------------
scala> val rdd1 = sc.parallelize(List(("a", 1), ("b", 2), ("b", 2), ("c", 2), ("c", 1)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:21

计算出key值相同的元组的个数，并用元组的形式表现出来
scala> rdd1.countByKey
res0: scala.collection.Map[String,Long] = Map(b -> 2, a -> 1, c -> 2)           

按照value值进行计算相同元组数量时，应注意即使value值相同，但key值不相同的也不能算作一类，，当value值相同，key也相同时才能算作一类
scala> rdd1.countByValue
res1: scala.collection.Map[(String, Int),Long] = Map((b,2) -> 2, (c,2) -> 1, (a,1) -> 1, (c,1) -> 1)

-----------------------------------------------------------------------------------------------------------
---练习15(filterByRange)
-----------------------------------------------------------------------------------------------------------

scala> val rdd1 = sc.parallelize(List(("e", 5), ("c", 3), ("d", 4), ("c", 2), ("a", 1)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[6] at parallelize at <console>:21

将元组中第一个元素与filterByRange参数相同的元组取出
scala> val rdd2 = rdd1.filterByRange("c", "d")
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[7] at filterByRange at <console>:23

scala> rdd2.collect
res2: Array[(String, Int)] = Array((c,3), (d,4), (c,2))


-----------------------------------------------------------------------------------------------------------
---练习16(flatMapValues)
-----------------------------------------------------------------------------------------------------------

scala> val rdd3=sc.parallelize(List(("a","1 2"),("b","3 4")))
rdd3: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[32] at parallelize at <console>:27

scala> rdd3.flatMapValues(_.split(" "))
res22: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[33] at flatMapValues at <console>:30

scala> res22.collect
res23: Array[(String, String)] = Array((a,1), (a,2), (b,3), (b,4))


-----------------------------------------------------------------------------------------------------------
---练习17(foldByKey)
-----------------------------------------------------------------------------------------------------------
scala> val rdd2 = rdd1.map(x => (x.length, x))
rdd2: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[11] at map at <console>:23

scala> rdd2.collect
res5: Array[(Int, String)] = Array((3,dog), (4,wolf), (3,cat), (4,bear))        

scala> val rdd3 = rdd2.foldByKey("")(_+_).collect
rdd3: Array[(Int, String)] = Array((4,bearwolf), (3,dogcat))

由下面这个例子可以说明foldByKey的第一个参数为初始值，第二个参数是对相同key值的元组的value运算

scala> val rdd3=rdd2.foldByKey("haha")(_+_)
rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[37] at foldByKey at <console>:31

scala> rdd3.collect
res27: Array[(Int, String)] = Array((4,hahabearhahawolf), (3,hahadoghahacat))

-----------------------------------------------------------------------------------------------------------
---练习18(foreachPartition)
-----------------------------------------------------------------------------------------------------------
在idea中运行
val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)
参数X代表分区，本例中X分别代表分区0,1,2       x.reduce(_ + _)代表对分区X中的元素累加
rdd1.foreachPartition(x => println(x.reduce(_ + _)))

结果：6   15   24


-----------------------------------------------------------------------------------------------------------
---练习19(keyBy)
-----------------------------------------------------------------------------------------------------------
scala> val rdd1 = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[14] at parallelize at <console>:21

对每个元素进行操作，分别计算出每个元素的长度，然后生成元组：(元素长度，元素)
scala> val rdd2 = rdd1.keyBy(_.length)
rdd2: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[15] at keyBy at <console>:23

scala> rdd2.collect
res7: Array[(Int, String)] = Array((3,dog), (6,salmon), (6,salmon), (3,rat), (8,elephant))


-----------------------------------------------------------------------------------------------------------
---练习20(keys，values)
-----------------------------------------------------------------------------------------------------------
scala> val rdd1=sc.parallelize(List("dog","tiger","lion","panther","eagle"),2)
rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[40] at parallelize at <console>:27

scala> val rdd2=rdd1.map(x=>(x.length,x))
rdd2: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[41] at map at <console>:29

scala> rdd2.collect
res29: Array[(Int, String)] = Array((3,dog), (5,tiger), (4,lion), (7,panther), (5,eagle))

将RDD元组中的key值取出
scala> rdd2.keys.collect
res30: Array[Int] = Array(3, 5, 4, 7, 5)  

将RDD元组中的value值取出
scala> rdd2.values.collect
res31: Array[String] = Array(dog, tiger, lion, panther, eagle)  

-----------------------------------------------------------------------------------------------------------
---练习21(coalesce)
-----------------------------------------------------------------------------------------------------------
scala> val rdd1=sc.parallelize(1 to 10,3)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[55] at parallelize at <console>:27

scala> rdd1.partitions.length
res42: Int = 3

重新分区后生成新的RDD  coalesce(分区数，是否分区)，，其中第二个参数不指定默认是false
scala> val rdd2=rdd1.coalesce(2)
rdd2: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[56] at coalesce at <console>:29

scala> rdd2.partitions.length
res43: Int = 2

scala> rdd1.partitions.length
res40: Int = 3         分区后产生了新的RDD，但是rdd1的分区并没有变化

scala> val rdd2=rdd1.coalesce(5)
rdd2: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[57] at coalesce at <console>:29

scala> rdd2.partitions.length
res44: Int = 3     分区没有变成5，是因为没有将原RDD进行shuffle


scala> val rdd2=rdd1.coalesce(5,true)
rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[40] at coalesce at <console>:23

scala> rdd2.partitions.length
res16: Int = 5     变了，因为上面有个true，就是发生shuffle


-----------------------------------------------------------------------------------------------------------
---练习22(repartition)
-----------------------------------------------------------------------------------------------------------

scala> val rdd1 = sc.parallelize(1 to 10, 3)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at <console>:21

scala> val rdd2=rdd1.repartition(1)
rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[30] at repartition at <console>:23

scala> rdd2.partitions.length
res12: Int = 1


scala> val rdd2=rdd1.repartition(5)
rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[34] at repartition at <console>:23

scala> rdd2.partitions.length
res13: Int = 5



-----------------------------------------------------------------------------------------------------------
---练习23(partitionBy)  (参数是通过一个对象的形式实现重新分区)
-----------------------------------------------------------------------------------------------------------

scala> val rdd2=sc.parallelize(List(("jerry",2),("tom",3),("shuke",4)),3)
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[41] at parallelize at <console>:21

scala> rdd2.partitionBy(new org.apache.spark.HashPartitioner(3))
res17: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[42] at partitionBy at <console>:24

scala> res17.partitions.length
res18: Int = 3

分区可变大
scala> rdd2.partitionBy(new org.apache.spark.HashPartitioner(4))
res19: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[43] at partitionBy at <console>:24

scala> res19.partitions.length
res20: Int = 4
分区可变小
scala> rdd2.partitionBy(new org.apache.spark.HashPartitioner(2))
res21: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[44] at partitionBy at <console>:24

scala> res21.partitions.length
res22: Int = 2


-----------------------------------------------------------------------------------------------------------
---练习24(collectAsMap)  
-----------------------------------------------------------------------------------------------------------
scala> val rdd = sc.parallelize(List(("a", 1), ("b", 2)))
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[45] at parallelize at <console>:21

scala> rdd.collectAsMap
res23: scala.collection.Map[String,Int] = Map(b -> 2, a -> 1)                   

```

