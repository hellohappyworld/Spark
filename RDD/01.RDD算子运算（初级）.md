#### RDD算子运算（初级）

```scala
**************************************************(常用算子)**************************************************
---注意一开始这些算子不会执行，直到res0.collect执行后上述算子才开始执行
---求元素数量
[hadoop@min1 spark-1.6.3]$ ./bin/spark-shell \
> --master spark://min1:7077

scala> sc.textFile("hdfs://min1:9000/wc").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
res0: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:28

scala> res0.collect
res1: Array[(String, Int)] = Array((tom,4), (hello,8), (jerry,2), (hanmeimei,1), (lilei,1))

---练习1
scala> val arr=Array(5,6,4,7,2,4,7,1,10)
arr: Array[Int] = Array(5, 6, 4, 7, 2, 4, 7, 1, 10)

创建RDD数据类型
scala> val rdd1=sc.parallelize(arr)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at <console>:29

将rdd1中的每个元素乘以2然后升序排序
scala> val rdd2=rdd1.map(_*2).sortBy(x => x)
rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[11] at sortBy at <console>:31

scala> rdd2.collect
res7: Array[Int] = Array(2, 4, 8, 8, 10, 12, 14, 14, 20)

将rdd2元素中值大于10的元素过滤出来
scala> rdd2.filter(_ >= 10)
res2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[12] at filter at <console>:34

scala> res2.collect
res3: Array[Int] = Array(10, 12, 14, 14, 20)   

---练习2
scala> val rdd1=sc.parallelize(Array("a b c","d e f","h i j"))
rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at <console>:27

scala> rdd1.collect
res2: Array[String] = Array(a b c, d e f, h i j) 
将rdd1中的每一个数据先切分后压平
scala> val rdd2=rdd1.flatMap(_.split(" "))
rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at flatMap at <console>:29

scala> rdd2.collect
res3: Array[String] = Array(a, b, c, d, e, f, h, i, j)


---练习3
scala> val rdd1=sc.parallelize(List(List("a b c","a b b"),List("e f g","a f g"),List("h i j","a a b")))
rdd1: org.apache.spark.rdd.RDD[List[String]] = ParallelCollectionRDD[4] at parallelize at <console>:27

scala> rdd1.collect
res4: Array[List[String]] = Array(List(a b c, a b b), List(e f g, a f g), List(h i j, a a b))

将rdd1中的数据先切分后压平  第一个flatMap获取的是整个的List，第二个_.flatMap代表的对大的List中的每个小的List执行操作  _.split(" ")代表将每个小的List里面的每个元素进行切分
scala> val rdd2=rdd1.flatMap(_.flatMap(_.split(" ")))
rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at flatMap at <console>:29

scala> rdd2.collect
res5: Array[String] = Array(a, b, c, a, b, b, e, f, g, a, f, g, h, i, j, a, a, b)

---练习4
scala> val rdd1=sc.parallelize(List(5,6,4,3))
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at <console>:27

scala> rdd1.collect
res6: Array[Int] = Array(5, 6, 4, 3)

scala> val rdd2=sc.parallelize(List(1,2,3,4))
rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at <console>:27

scala> rdd2.collect
res7: Array[Int] = Array(1, 2, 3, 4) 

对rdd1和rdd2求并集
scala> val rdd3=rdd1.union(rdd2)
rdd3: org.apache.spark.rdd.RDD[Int] = UnionRDD[8] at union at <console>:31

scala> rdd3.collect
res8: Array[Int] = Array(5, 6, 4, 3, 1, 2, 3, 4)  

对rdd1和rdd2求交集
scala> val rdd4=rdd1.intersection(rdd2)
rdd4: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[14] at intersection at <console>:31

scala> rdd4.collect
res9: Array[Int] = Array(4, 3)   

对rdd3中数据去重，并产生新的rdd，原rdd中数据不变
scala> rdd3.distinct
res9: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[28] at distinct at <console>:34

scala> res9.collect
res10: Array[Int] = Array(4, 1, 5, 6, 2, 3)

scala> rdd3.collect
res14: Array[Int] = Array(5, 6, 4, 3, 1, 2, 3, 4)

---练习5

将含有元组的List创建成RDD
scala> val rdd1=sc.parallelize(List(("tom",1),("jerry",3),("kitty",2)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[18] at parallelize at <console>:27

scala> rdd1.collect
res15: Array[(String, Int)] = Array((tom,1), (jerry,3), (kitty,2))

scala> val rdd2=sc.parallelize(List(("jerry",2),("tom",1),("shuke",2)))
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[19] at parallelize at <console>:27

scala> rdd2.collect
res16: Array[(String, Int)] = Array((jerry,2), (tom,1), (shuke,2))

求join，由于rdd1中有kitty,而rdd2中没有kitty，则去除该元素
scala> val rdd3=rdd1 join rdd2
rdd3: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[33] at join at <console>:31

scala> rdd3.collect
res11: Array[(String, (Int, Int))] = Array((tom,(1,1)), (jerry,(3,2)))
同样道理，在join操作时，因为rdd2中有shuke,而rdd1中没有shuke，则去除掉
scala> val rdd3=rdd2.join(rdd1)
rdd3: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[25] at join at <console>:31

scala> rdd3.collect
res19: Array[(String, (Int, Int))] = Array((tom,(1,1)), (jerry,(2,3)))  

---左连接
scala> val rdd3=rdd1 leftOuterJoin rdd2
rdd3: org.apache.spark.rdd.RDD[(String, (Int, Option[Int]))] = MapPartitionsRDD[36] at leftOuterJoin at <console>:31

scala> rdd3.collect
res12: Array[(String, (Int, Option[Int]))] = Array((tom,(1,Some(1))), (jerry,(3,Some(2))), (kitty,(2,None)))

--右连接
scala> val rdd3=rdd1 rightOuterJoin rdd2
rdd3: org.apache.spark.rdd.RDD[(String, (Option[Int], Int))] = MapPartitionsRDD[39] at rightOuterJoin at <console>:31

scala> rdd3.collect
res13: Array[(String, (Option[Int], Int))] = Array((tom,(Some(1),1)), (jerry,(Some(3),2)), (shuke,(None,2)))

---练习6
求并集
scala> val rdd4=rdd1 union rdd2
rdd4: org.apache.spark.rdd.RDD[(String, Int)] = UnionRDD[8] at union at <console>:31

scala> rdd4.collect
res3: Array[(String, Int)] = Array((tom,1), (jerry,3), (kitty,2), (jerry,2), (tom,1), (shuke,2))

按key进行分组
scala> rdd4.groupByKey()
res16: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[41] at groupByKey at <console>:34

scala> res16.collect
res17: Array[(String, Iterable[Int])] = Array((tom,CompactBuffer(1, 1)), (jerry,CompactBuffer(3, 2)), (shuke,CompactBuffer(2)), (kitty,CompactBuffer(2)))


分别使用groupByKey和reduceByKey实现单词计数
groupByKey是按照RDD元组中的key对同一类元组进行分类，，reduceByKey是对RDD中同一类元组分类后，再运用一些函数对每个元组内部的数据进行操作
scala> rdd4.groupByKey().mapValues(_.sum).collect
res10: Array[(String, Int)] = Array((tom,2), (jerry,5), (shuke,2), (kitty,2)) 

scala> rdd4.reduceByKey(_+_).collect
res12: Array[(String, Int)] = Array((tom,2), (jerry,5), (shuke,2), (kitty,2))

---练习7
scala> val rdd1=sc.parallelize(List(("tom",1),("tom",2),("jerry",3),("kitty",2)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[14] at parallelize at <console>:27

scala> rdd1.collect
res14: Array[(String, Int)] = Array((tom,1), (tom,2), (jerry,3), (kitty,2))

scala> val rdd2=sc.parallelize(List(("jerry",2),("tom",1),("shuke",2)))
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[15] at parallelize at <console>:27

scala> rdd2.collect
res15: Array[(String, Int)] = Array((jerry,2), (tom,1), (shuke,2))

注意：cogroup与groupByKey的区别    
scala> val rdd3=rdd1 cogroup rdd2
rdd3: org.apache.spark.rdd.RDD[(String, (Iterable[Int], Iterable[Int]))] = MapPartitionsRDD[17] at cogroup at <console>:31

scala> rdd3.collect
res16: Array[(String, (Iterable[Int], Iterable[Int]))] = Array((tom,(CompactBuffer(1, 2),CompactBuffer(1))), (jerry,(CompactBuffer(3),CompactBuffer(2))), (shuke,(CompactBuffer(),CompactBuffer(2))), (kitty,(CompactBuffer(2),CompactBuffer())))


---练习8
scala> val rdd1=sc.parallelize(List(1,2,3,4,5))
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[18] at parallelize at <console>:27

scala> rdd1.collect
res17: Array[Int] = Array(1, 2, 3, 4, 5)

scala> val rdd2=rdd1.reduce(_+_)
rdd2: Int = 15


---练习9

scala> val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 3), ("kitty", 2),  ("shuke", 1)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[19] at parallelize at <console>:27

scala> val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 3), ("shuke", 2), ("kitty", 5)))
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[20] at parallelize at <console>:27

scala> val rdd3=rdd1 union rdd2
rdd3: org.apache.spark.rdd.RDD[(String, Int)] = UnionRDD[21] at union at <console>:31

scala> rdd3.collect
res18: Array[(String, Int)] = Array((tom,1), (jerry,3), (kitty,2), (shuke,1), (jerry,2), (tom,3), (shuke,2), (kitty,5))

scala> val rdd4=rdd3.reduceByKey(_+_)
rdd4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[22] at reduceByKey at <console>:33

scala> rdd4.collect
res19: Array[(String, Int)] = Array((tom,4), (jerry,5), (shuke,3), (kitty,7)) 

首先通过rdd4.map将rdd4中元组kv颠倒，然后按照颠倒后的key值进行降序排序，排序后再将RDD中的kv颠倒过来
scala> val rdd5=rdd4.map(t => (t._2,t._1)).sortByKey(false).map(t => (t._2,t._1))
rdd5: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[31] at map at <console>:35

scala> rdd5.collect
res25: Array[(String, Int)] = Array((kitty,7), (jerry,5), (tom,4), (shuke,3)

---练习10
scala> val rdd1=sc.parallelize(List(("tom",1),("tom",2),("jerry",3),("kitty",2)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[35] at parallelize at <console>:27

scala> val rdd2=sc.parallelize(List(("jerry",2),("tom",1),("shuke",2)))
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[36] at parallelize at <console>:27

scala> val rdd3=rdd1 union rdd2
rdd3: org.apache.spark.rdd.RDD[(String, Int)] = UnionRDD[37] at union at <console>:31

scala> rdd3.collect
res29: Array[(String, Int)] = Array((tom,1), (tom,2), (jerry,3), (kitty,2), (jerry,2), (tom,1), (shuke,2))

scala> rdd3.groupByKey()
res30: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[38] at groupByKey at <console>:34

scala> res30.collect
res31: Array[(String, Iterable[Int])] = Array((tom,CompactBuffer(1, 2, 1)), (jerry,CompactBuffer(3, 2)), (shuke,CompactBuffer(2)), (kitty,CompactBuffer(2)))

注意：map和mapValues的区别，，map中的x代表RDD中的一个元组，然后对元组内部第二部分(也就是那个数字元组)进行计算
      mapValues中的下划线代表RDD中的每一个元组内的第二部分(也就是那个数字元组)，，然后对这个数字元组进行redece运算
scala> res30.map(x => (x._1,x._2.sum)).collect
res32: Array[(String, Int)] = Array((tom,4), (jerry,5), (shuke,2), (kitty,2))   

scala> res30.mapValues(_.reduce(_+_)).collect
res33: Array[(String, Int)] = Array((tom,4), (jerry,5), (shuke,2), (kitty,2))


---练习11
取前几个数，以降序排序

scala> val rdd1=sc.parallelize(List(("tom",1),("jerry",3),("kitty",2),("jerry",2),("tom",1),("shuke",2)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[41] at parallelize at <console>:27

scala> rdd1.collect
res34: Array[(String, Int)] = Array((tom,1), (jerry,3), (kitty,2), (jerry,2), (tom,1), (shuke,2))

scala> rdd1.top(3)
res35: Array[(String, Int)] = Array((tom,1), (tom,1), (shuke,2))            

            

scala> val rdd1=sc.parallelize(Array(2,1,4,56,3,7,3,4))
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[43] at parallelize at <console>:27

scala> rdd1.collect
res36: Array[Int] = Array(2, 1, 4, 56, 3, 7, 3, 4)

scala> rdd1.top(4)
res37: Array[Int] = Array(56, 7, 4, 4)


```

